# 03. HDFS 실전 시나리오 실습

**태그**: #hadoop #hdfs #실전 #시나리오 #hands-on
**소요 시간**: 3-4시간
**난이도**: 중급

## 목표

실무에서 마주칠 수 있는 실제 시나리오를 통해 HDFS의 활용법을 익힙니다.

**완료 기준**:
- [ ] 시나리오 1: RDS 백업 데이터를 HDFS에 저장 완료
- [ ] 시나리오 2: 애플리케이션 로그 수집 및 분석 완료
- [ ] 시나리오 3: 대용량 데이터 처리 파이프라인 구현 완료
- [ ] 시나리오 4: 데이터 아카이빙 시스템 구축 완료

## 준비 사항

```bash
# Hadoop 클러스터 상태 확인
docker exec hadoop-namenode hdfs dfsadmin -report

# 작업 디렉토리 생성
hdfs dfs -mkdir -p /project/scenarios
```

---

## 시나리오 1: RDS 백업 데이터를 HDFS에 저장

### 배경
MySQL RDS의 일일 백업 데이터를 HDFS에 저장하여 장기 보관 및 분석에 활용합니다.

### 목표
- MySQL 덤프 파일을 HDFS에 저장
- 날짜별로 디렉토리 구조 생성
- 압축 및 replication 설정
- 백업 자동화 스크립트 작성

### Step 1: MySQL 백업 데이터 생성 (시뮬레이션)

```bash
# 로컬 백업 디렉토리 생성
mkdir -p /tmp/mysql_backup

# 샘플 SQL 덤프 파일 생성
cat <<EOF > /tmp/mysql_backup/users_20251117.sql
-- MySQL dump
-- Host: rds.amazonaws.com    Database: production

DROP TABLE IF EXISTS \`users\`;
CREATE TABLE \`users\` (
  \`id\` int NOT NULL AUTO_INCREMENT,
  \`username\` varchar(50) NOT NULL,
  \`email\` varchar(100) NOT NULL,
  \`created_at\` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  PRIMARY KEY (\`id\`)
);

INSERT INTO \`users\` VALUES
(1,'alice','alice@example.com','2025-01-01 10:00:00'),
(2,'bob','bob@example.com','2025-01-02 11:00:00'),
(3,'charlie','charlie@example.com','2025-01-03 12:00:00');
EOF

cat <<EOF > /tmp/mysql_backup/orders_20251117.sql
-- MySQL dump
-- Host: rds.amazonaws.com    Database: production

DROP TABLE IF EXISTS \`orders\`;
CREATE TABLE \`orders\` (
  \`id\` int NOT NULL AUTO_INCREMENT,
  \`user_id\` int NOT NULL,
  \`amount\` decimal(10,2) NOT NULL,
  \`status\` varchar(20) NOT NULL,
  \`created_at\` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  PRIMARY KEY (\`id\`)
);

INSERT INTO \`orders\` VALUES
(1,1,99.99,'completed','2025-01-05 14:00:00'),
(2,2,149.50,'pending','2025-01-06 15:00:00'),
(3,1,200.00,'completed','2025-01-07 16:00:00');
EOF

# 파일 크기 확인
ls -lh /tmp/mysql_backup/
```

### Step 2: HDFS에 백업 디렉토리 구조 생성

```bash
# 연도/월/일 기반 디렉토리 구조
BACKUP_DATE=$(date +%Y/%m/%d)
hdfs dfs -mkdir -p /backup/mysql/${BACKUP_DATE}

# 확인
hdfs dfs -ls -R /backup
```

**디렉토리 구조**:
```
/backup/mysql/
└── 2025/
    └── 11/
        └── 17/
            ├── users_20251117.sql
            └── orders_20251117.sql
```

### Step 3: 백업 파일 압축 및 업로드

```bash
# SQL 파일 압축
cd /tmp/mysql_backup
gzip users_20251117.sql
gzip orders_20251117.sql

# HDFS에 업로드
BACKUP_DATE=$(date +%Y/%m/%d)
hdfs dfs -put /tmp/mysql_backup/*.sql.gz /backup/mysql/${BACKUP_DATE}/

# 업로드 확인
hdfs dfs -ls -h /backup/mysql/${BACKUP_DATE}/
```

### Step 4: Replication 및 권한 설정

```bash
# Replication factor 설정 (중요 데이터이므로 3으로 설정)
hdfs dfs -setrep 3 /backup/mysql/${BACKUP_DATE}/*

# 읽기 전용 권한 설정
hdfs dfs -chmod 444 /backup/mysql/${BACKUP_DATE}/*

# 확인
hdfs dfs -ls /backup/mysql/${BACKUP_DATE}/
```

### Step 5: 백업 자동화 스크립트

`mysql_backup_to_hdfs.sh`:

```bash
#!/bin/bash

# 설정
MYSQL_HOST="rds.amazonaws.com"
MYSQL_USER="admin"
MYSQL_PASS="password"
DATABASES=("production" "analytics")
BACKUP_LOCAL_DIR="/tmp/mysql_backup"
BACKUP_DATE=$(date +%Y/%m/%d)
HDFS_BACKUP_DIR="/backup/mysql/${BACKUP_DATE}"

# 로컬 백업 디렉토리 생성
mkdir -p ${BACKUP_LOCAL_DIR}
cd ${BACKUP_LOCAL_DIR}

# MySQL 덤프
for db in "${DATABASES[@]}"; do
    echo "Backing up database: ${db}"
    mysqldump -h ${MYSQL_HOST} -u ${MYSQL_USER} -p${MYSQL_PASS} ${db} > ${db}_$(date +%Y%m%d).sql

    # 압축
    gzip ${db}_$(date +%Y%m%d).sql
done

# HDFS 디렉토리 생성
docker exec hadoop-namenode hdfs dfs -mkdir -p ${HDFS_BACKUP_DIR}

# HDFS에 업로드
docker exec hadoop-namenode hdfs dfs -put ${BACKUP_LOCAL_DIR}/*.sql.gz ${HDFS_BACKUP_DIR}/

# Replication 설정
docker exec hadoop-namenode hdfs dfs -setrep 3 ${HDFS_BACKUP_DIR}/*

# 권한 설정
docker exec hadoop-namenode hdfs dfs -chmod 444 ${HDFS_BACKUP_DIR}/*

# 7일 이상 된 로컬 백업 삭제
find ${BACKUP_LOCAL_DIR} -name "*.sql.gz" -mtime +7 -delete

echo "Backup completed: ${HDFS_BACKUP_DIR}"
```

### Step 6: 백업 검증

```bash
# 백업 파일 확인
hdfs dfs -ls -R /backup/mysql/

# 파일 무결성 확인
hdfs dfs -checksum /backup/mysql/${BACKUP_DATE}/users_20251117.sql.gz

# 복원 테스트
hdfs dfs -get /backup/mysql/${BACKUP_DATE}/users_20251117.sql.gz /tmp/
gunzip /tmp/users_20251117.sql.gz
head -20 /tmp/users_20251117.sql
```

---

## 시나리오 2: 애플리케이션 로그 수집 및 분석

### 배경
여러 서버에서 생성되는 애플리케이션 로그를 HDFS에 중앙 집중화하여 분석합니다.

### 목표
- 여러 서버의 로그를 HDFS에 수집
- 시간별/서버별 디렉토리 구조 생성
- 로그 파싱 및 필터링
- 기본 통계 분석

### Step 1: 샘플 로그 데이터 생성

```bash
# 로그 디렉토리 생성
mkdir -p /tmp/app_logs/{server1,server2,server3}

# server1 로그 생성
cat <<EOF > /tmp/app_logs/server1/application.log
2025-11-17 10:00:01 INFO  [main] Starting application...
2025-11-17 10:00:05 INFO  [main] Database connection established
2025-11-17 10:05:12 WARN  [worker-1] Connection timeout, retrying...
2025-11-17 10:05:15 INFO  [worker-1] Connection successful
2025-11-17 10:10:23 ERROR [worker-2] Failed to process request: NullPointerException
2025-11-17 10:10:25 ERROR [worker-2] Stack trace: at com.example.Service.process(Service.java:42)
2025-11-17 10:15:30 INFO  [main] Health check passed
2025-11-17 10:20:45 WARN  [worker-3] High memory usage: 85%
EOF

# server2 로그 생성
cat <<EOF > /tmp/app_logs/server2/application.log
2025-11-17 10:00:02 INFO  [main] Starting application...
2025-11-17 10:00:06 INFO  [main] Database connection established
2025-11-17 10:07:18 ERROR [worker-1] Database query failed
2025-11-17 10:07:20 INFO  [worker-1] Retrying query...
2025-11-17 10:07:22 INFO  [worker-1] Query successful
2025-11-17 10:15:31 INFO  [main] Health check passed
EOF

# server3 로그 생성
cat <<EOF > /tmp/app_logs/server3/application.log
2025-11-17 10:00:03 INFO  [main] Starting application...
2025-11-17 10:00:07 INFO  [main] Database connection established
2025-11-17 10:12:40 WARN  [worker-1] Slow query detected: 5.2s
2025-11-17 10:15:32 INFO  [main] Health check passed
2025-11-17 10:25:55 ERROR [worker-2] Unexpected error: FileNotFoundException
EOF
```

### Step 2: HDFS에 로그 디렉토리 구조 생성

```bash
# 날짜/시간 기반 구조
LOG_DATE=$(date +%Y%m%d)
LOG_HOUR=$(date +%H)

hdfs dfs -mkdir -p /logs/application/${LOG_DATE}/${LOG_HOUR}/server1
hdfs dfs -mkdir -p /logs/application/${LOG_DATE}/${LOG_HOUR}/server2
hdfs dfs -mkdir -p /logs/application/${LOG_DATE}/${LOG_HOUR}/server3

# 확인
hdfs dfs -ls -R /logs
```

### Step 3: 로그 파일 업로드

```bash
# 각 서버의 로그를 HDFS에 업로드
LOG_DATE=$(date +%Y%m%d)
LOG_HOUR=$(date +%H)

docker exec hadoop-namenode bash -c "hdfs dfs -put /tmp/app_logs/server1/application.log /logs/application/${LOG_DATE}/${LOG_HOUR}/server1/"
docker exec hadoop-namenode bash -c "hdfs dfs -put /tmp/app_logs/server2/application.log /logs/application/${LOG_DATE}/${LOG_HOUR}/server2/"
docker exec hadoop-namenode bash -c "hdfs dfs -put /tmp/app_logs/server3/application.log /logs/application/${LOG_DATE}/${LOG_HOUR}/server3/"

# 확인
hdfs dfs -ls -R /logs/application/${LOG_DATE}/
```

### Step 4: 로그 분석 - 에러 추출

```bash
# 모든 서버의 ERROR 로그 추출
LOG_DATE=$(date +%Y%m%d)
hdfs dfs -cat /logs/application/${LOG_DATE}/**/application.log | grep ERROR > /tmp/errors.log

# 결과 확인
cat /tmp/errors.log

# HDFS에 저장
hdfs dfs -put /tmp/errors.log /logs/analysis/${LOG_DATE}/
```

### Step 5: 로그 통계 분석

```bash
# 로그 레벨별 카운트
LOG_DATE=$(date +%Y%m%d)
hdfs dfs -cat /logs/application/${LOG_DATE}/**/application.log | awk '{print $3}' | sort | uniq -c

# 서버별 로그 라인 수
for server in server1 server2 server3; do
    echo -n "$server: "
    hdfs dfs -cat /logs/application/${LOG_DATE}/**/${server}/application.log | wc -l
done

# 시간대별 에러 발생 횟수
hdfs dfs -cat /logs/application/${LOG_DATE}/**/application.log | grep ERROR | awk '{print $2}' | cut -d: -f1 | sort | uniq -c
```

**예상 출력**:
```
Log level counts:
      8 INFO
      3 WARN
      5 ERROR

Server log lines:
server1: 8
server2: 6
server3: 5

Errors by hour:
      2 10:05
      1 10:07
      2 10:10
```

### Step 6: 로그 수집 자동화

`collect_logs_to_hdfs.sh`:

```bash
#!/bin/bash

SERVERS=("server1" "server2" "server3")
LOG_DATE=$(date +%Y%m%d)
LOG_HOUR=$(date +%H)
HDFS_BASE="/logs/application/${LOG_DATE}/${LOG_HOUR}"

for server in "${SERVERS[@]}"; do
    echo "Collecting logs from ${server}..."

    # HDFS 디렉토리 생성
    docker exec hadoop-namenode hdfs dfs -mkdir -p ${HDFS_BASE}/${server}

    # 로그 파일 업로드 (실제 환경에서는 scp, rsync 등 사용)
    docker exec hadoop-namenode hdfs dfs -put /var/log/app/${server}/application.log ${HDFS_BASE}/${server}/

    echo "Logs from ${server} uploaded to HDFS"
done

# 에러 로그 분석
echo "Analyzing error logs..."
docker exec hadoop-namenode hdfs dfs -cat ${HDFS_BASE}/**/application.log | grep ERROR > /tmp/error_summary_${LOG_DATE}_${LOG_HOUR}.log

# 에러 요약 업로드
docker exec hadoop-namenode hdfs dfs -put /tmp/error_summary_${LOG_DATE}_${LOG_HOUR}.log /logs/analysis/${LOG_DATE}/

echo "Log collection and analysis completed"
```

---

## 시나리오 3: 대용량 데이터 처리 파이프라인

### 배경
외부 API에서 받은 대용량 JSON 데이터를 HDFS에 저장하고 변환합니다.

### 목표
- JSON 데이터를 HDFS에 저장
- 데이터 포맷 변환 (JSON → CSV)
- 파티셔닝 (날짜별, 카테고리별)
- 데이터 품질 검증

### Step 1: 샘플 JSON 데이터 생성

```bash
# 대용량 JSON 데이터 시뮬레이션
mkdir -p /tmp/api_data

cat <<EOF > /tmp/api_data/products_20251117.json
{"id": 1, "name": "Laptop", "category": "Electronics", "price": 1200.00, "stock": 50, "date": "2025-11-17"}
{"id": 2, "name": "Mouse", "category": "Electronics", "price": 25.00, "stock": 200, "date": "2025-11-17"}
{"id": 3, "name": "Desk", "category": "Furniture", "price": 350.00, "stock": 30, "date": "2025-11-17"}
{"id": 4, "name": "Chair", "category": "Furniture", "price": 180.00, "stock": 45, "date": "2025-11-17"}
{"id": 5, "name": "Monitor", "category": "Electronics", "price": 300.00, "stock": 75, "date": "2025-11-17"}
{"id": 6, "name": "Keyboard", "category": "Electronics", "price": 80.00, "stock": 150, "date": "2025-11-17"}
{"id": 7, "name": "Bookshelf", "category": "Furniture", "price": 220.00, "stock": 20, "date": "2025-11-17"}
{"id": 8, "name": "Lamp", "category": "Furniture", "price": 45.00, "stock": 100, "date": "2025-11-17"}
EOF

# 데이터 확인
cat /tmp/api_data/products_20251117.json
```

### Step 2: Raw 데이터를 HDFS에 저장

```bash
# Raw 데이터 디렉토리
hdfs dfs -mkdir -p /data/raw/products/2025/11/17

# 업로드
hdfs dfs -put /tmp/api_data/products_20251117.json /data/raw/products/2025/11/17/

# 확인
hdfs dfs -cat /data/raw/products/2025/11/17/products_20251117.json
```

### Step 3: JSON을 CSV로 변환

```bash
# jq를 사용한 JSON to CSV 변환 (로컬에서)
cat /tmp/api_data/products_20251117.json | jq -r '[.id, .name, .category, .price, .stock, .date] | @csv' > /tmp/products_20251117.csv

# CSV 헤더 추가
echo "id,name,category,price,stock,date" | cat - /tmp/products_20251117.csv > /tmp/products_20251117_with_header.csv

# 결과 확인
cat /tmp/products_20251117_with_header.csv

# HDFS에 업로드
hdfs dfs -mkdir -p /data/processed/products/2025/11/17
hdfs dfs -put /tmp/products_20251117_with_header.csv /data/processed/products/2025/11/17/products.csv
```

### Step 4: 카테고리별 파티셔닝

```bash
# Electronics 제품만 필터링
hdfs dfs -cat /data/processed/products/2025/11/17/products.csv | grep "Electronics" > /tmp/electronics.csv

# Furniture 제품만 필터링
hdfs dfs -cat /data/processed/products/2025/11/17/products.csv | grep "Furniture" > /tmp/furniture.csv

# 파티션 디렉토리 생성 및 업로드
hdfs dfs -mkdir -p /data/partitioned/products/category=Electronics/2025/11/17
hdfs dfs -mkdir -p /data/partitioned/products/category=Furniture/2025/11/17

hdfs dfs -put /tmp/electronics.csv /data/partitioned/products/category=Electronics/2025/11/17/
hdfs dfs -put /tmp/furniture.csv /data/partitioned/products/category=Furniture/2025/11/17/

# 확인
hdfs dfs -ls -R /data/partitioned/products/
```

### Step 5: 데이터 품질 검증

```bash
# Raw 데이터 라인 수
RAW_COUNT=$(hdfs dfs -cat /data/raw/products/2025/11/17/products_20251117.json | wc -l)
echo "Raw records: ${RAW_COUNT}"

# Processed 데이터 라인 수 (헤더 제외)
PROCESSED_COUNT=$(hdfs dfs -cat /data/processed/products/2025/11/17/products.csv | tail -n +2 | wc -l)
echo "Processed records: ${PROCESSED_COUNT}"

# 카테고리별 레코드 수
ELECTRONICS_COUNT=$(hdfs dfs -cat /data/partitioned/products/category=Electronics/2025/11/17/electronics.csv | wc -l)
FURNITURE_COUNT=$(hdfs dfs -cat /data/partitioned/products/category=Furniture/2025/11/17/furniture.csv | wc -l)
TOTAL_PARTITIONED=$((ELECTRONICS_COUNT + FURNITURE_COUNT))

echo "Electronics: ${ELECTRONICS_COUNT}"
echo "Furniture: ${FURNITURE_COUNT}"
echo "Total partitioned: ${TOTAL_PARTITIONED}"

# 데이터 일관성 검증
if [ ${RAW_COUNT} -eq ${PROCESSED_COUNT} ] && [ ${RAW_COUNT} -eq ${TOTAL_PARTITIONED} ]; then
    echo "✓ Data validation passed"
else
    echo "✗ Data validation failed"
fi
```

### Step 6: 데이터 처리 파이프라인 스크립트

`data_pipeline.sh`:

```bash
#!/bin/bash

DATE=$(date +%Y/%m/%d)
DATE_COMPACT=$(date +%Y%m%d)
RAW_DIR="/data/raw/products/${DATE}"
PROCESSED_DIR="/data/processed/products/${DATE}"
PARTITIONED_DIR="/data/partitioned/products"

echo "Starting data pipeline for ${DATE_COMPACT}"

# 1. Raw 데이터 업로드
docker exec hadoop-namenode hdfs dfs -mkdir -p ${RAW_DIR}
docker exec hadoop-namenode hdfs dfs -put /tmp/api_data/products_${DATE_COMPACT}.json ${RAW_DIR}/

# 2. JSON to CSV 변환
docker exec hadoop-namenode hdfs dfs -cat ${RAW_DIR}/products_${DATE_COMPACT}.json | \
    jq -r '[.id, .name, .category, .price, .stock, .date] | @csv' > /tmp/products_${DATE_COMPACT}.csv

# 3. 헤더 추가
echo "id,name,category,price,stock,date" | cat - /tmp/products_${DATE_COMPACT}.csv > /tmp/products_${DATE_COMPACT}_with_header.csv

# 4. Processed 데이터 업로드
docker exec hadoop-namenode hdfs dfs -mkdir -p ${PROCESSED_DIR}
docker exec hadoop-namenode hdfs dfs -put /tmp/products_${DATE_COMPACT}_with_header.csv ${PROCESSED_DIR}/products.csv

# 5. 카테고리별 파티셔닝
for category in Electronics Furniture; do
    docker exec hadoop-namenode hdfs dfs -cat ${PROCESSED_DIR}/products.csv | grep "${category}" > /tmp/${category,,}.csv
    docker exec hadoop-namenode hdfs dfs -mkdir -p ${PARTITIONED_DIR}/category=${category}/${DATE}
    docker exec hadoop-namenode hdfs dfs -put /tmp/${category,,}.csv ${PARTITIONED_DIR}/category=${category}/${DATE}/
done

echo "Data pipeline completed successfully"
```

---

## 시나리오 4: 데이터 아카이빙 시스템

### 배경
30일 이상 된 오래된 데이터를 압축하여 아카이브하고, 90일 이상 된 데이터는 삭제합니다.

### 목표
- 오래된 데이터 식별
- 압축 및 아카이브
- 자동 정리 정책 구현

### Step 1: 테스트 데이터 생성 (다양한 날짜)

```bash
# 여러 날짜의 데이터 생성
for days_ago in 1 35 95; do
    DATE=$(date -d "${days_ago} days ago" +%Y/%m/%d 2>/dev/null || date -v -${days_ago}d +%Y/%m/%d)
    DATE_COMPACT=$(date -d "${days_ago} days ago" +%Y%m%d 2>/dev/null || date -v -${days_ago}d +%Y%m%d)

    # HDFS에 디렉토리 생성
    docker exec hadoop-namenode hdfs dfs -mkdir -p /data/transactions/${DATE}

    # 샘플 데이터 생성
    echo "transaction_${days_ago}_days_ago" > /tmp/transaction_${DATE_COMPACT}.txt
    docker exec hadoop-namenode hdfs dfs -put /tmp/transaction_${DATE_COMPACT}.txt /data/transactions/${DATE}/
done

# 확인
hdfs dfs -ls -R /data/transactions/
```

### Step 2: 아카이브 대상 식별

```bash
# 30일 이상 된 디렉토리 찾기
ARCHIVE_DATE=$(date -d "30 days ago" +%Y-%m-%d 2>/dev/null || date -v -30d +%Y-%m-%d)

echo "Finding directories older than ${ARCHIVE_DATE}..."

# HDFS에서 오래된 파일 찾기 (find 명령어 대신 스크립트 사용)
hdfs dfs -ls -R /data/transactions/ | awk '{print $8}' | while read path; do
    if [ ! -z "$path" ]; then
        echo "$path"
    fi
done
```

### Step 3: 아카이브 스크립트

`archive_old_data.sh`:

```bash
#!/bin/bash

ARCHIVE_DAYS=30
DELETE_DAYS=90
ARCHIVE_DIR="/archive/transactions"
DATA_DIR="/data/transactions"

# 아카이브 날짜 계산
ARCHIVE_DATE=$(date -d "${ARCHIVE_DAYS} days ago" +%Y%m%d 2>/dev/null || date -v -${ARCHIVE_DAYS}d +%Y%m%d)
DELETE_DATE=$(date -d "${DELETE_DAYS} days ago" +%Y%m%d 2>/dev/null || date -v -${DELETE_DAYS}d +%Y%m%d)

echo "Archiving data older than ${ARCHIVE_DATE}"
echo "Deleting data older than ${DELETE_DATE}"

# 1. 30일 이상 된 데이터 아카이브
docker exec hadoop-namenode hdfs dfs -mkdir -p ${ARCHIVE_DIR}

# 2. 데이터 압축 및 아카이브 (실제로는 날짜 비교 로직 필요)
# 간단한 예제: 특정 디렉토리 아카이브
docker exec hadoop-namenode hdfs dfs -get /data/transactions/2025/10/13 /tmp/
tar -czf /tmp/transactions_20251013.tar.gz -C /tmp 2025/10/13
docker exec hadoop-namenode hdfs dfs -put /tmp/transactions_20251013.tar.gz ${ARCHIVE_DIR}/

# 3. 원본 데이터 삭제
docker exec hadoop-namenode hdfs dfs -rm -r /data/transactions/2025/10/13

# 4. 90일 이상 된 아카이브 삭제
# (실제 구현 시 날짜 비교 필요)

echo "Archival process completed"
```

---

## 실습 과제

### 과제 1: 멀티 소스 데이터 통합

3개의 서로 다른 소스(API, DB, File)에서 데이터를 받아 HDFS의 통합 디렉토리에 저장하는 파이프라인을 구축하세요.

**요구사항**:
- `/data/sources/api/`, `/data/sources/db/`, `/data/sources/file/` 각각에 데이터 저장
- 모든 데이터를 `/data/integrated/` 에 통합
- 소스별로 메타데이터 추가

### 과제 2: 로그 레벨별 분석 리포트

애플리케이션 로그를 분석하여 다음 정보를 추출하세요:
- 시간대별 ERROR 발생 빈도
- 서버별 WARN 비율
- 가장 자주 발생하는 에러 메시지 Top 5

### 과제 3: 데이터 품질 모니터링

매일 수집되는 데이터의 품질을 모니터링하는 스크립트를 작성하세요:
- 레코드 수 체크
- 필수 컬럼 존재 여부 확인
- 중복 데이터 탐지
- 품질 리포트를 `/data/quality_reports/` 에 저장

---

## 검증 체크리스트

- [ ] RDS 백업을 HDFS에 저장하고 복원할 수 있음
- [ ] 여러 서버의 로그를 HDFS에 중앙 집중화할 수 있음
- [ ] 로그에서 에러를 추출하고 분석할 수 있음
- [ ] JSON 데이터를 CSV로 변환하여 저장할 수 있음
- [ ] 데이터를 카테고리별로 파티셔닝할 수 있음
- [ ] 데이터 품질을 검증하는 스크립트를 작성할 수 있음
- [ ] 오래된 데이터를 아카이브하고 정리할 수 있음

## 다음 단계

실전 시나리오를 완료했다면 성능 및 장애 테스트를 진행하세요:
→ **[04-성능장애테스트.md](./04-성능장애테스트.md)**

---

**작성일**: 2025-11-17
**최종 수정**: 2025-11-17
