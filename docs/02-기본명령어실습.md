# 02. HDFS 기본 명령어 실습

**태그**: #hadoop #hdfs #cli #명령어 #hands-on
**소요 시간**: 2-3시간
**난이도**: 초급~중급

## 목표

HDFS의 핵심 명령어를 실습하며 분산 파일 시스템의 동작 방식을 이해합니다.

**완료 기준**:
- [ ] 파일/디렉토리 생성, 조회, 삭제 가능
- [ ] 파일 업로드/다운로드 가능
- [ ] Replication factor 조정 가능
- [ ] 블록 정보 및 메타데이터 확인 가능

## 준비 사항

환경 구축이 완료되었는지 확인:

```bash
# Hadoop 컨테이너 상태 확인
docker ps | grep hadoop

# HDFS 상태 확인
docker exec hadoop-namenode hdfs dfsadmin -report
```

## HDFS CLI 기본 사용법

HDFS 명령어는 다음 형식을 따릅니다:

```bash
hdfs dfs -<command> <arguments>
```

또는:

```bash
hadoop fs -<command> <arguments>
```

**참고**: `hdfs dfs`와 `hadoop fs`는 거의 동일하며, `hdfs dfs`가 더 권장됩니다.

## 실습 1: 디렉토리 관리

### 1.1 디렉토리 생성

```bash
# HDFS 컨테이너 접속
docker exec -it hadoop-namenode bash

# 단일 디렉토리 생성
hdfs dfs -mkdir /user

# 중첩 디렉토리 생성 (-p 옵션)
hdfs dfs -mkdir -p /user/hadoop/data/logs

# 생성 확인
hdfs dfs -ls /
hdfs dfs -ls /user
hdfs dfs -ls -R /user  # 재귀적으로 모든 하위 디렉토리 표시
```

**예상 출력**:
```
Found 1 items
drwxr-xr-x   - root supergroup          0 2025-11-17 10:00 /user
```

### 1.2 디렉토리 조회

```bash
# 루트 디렉토리 확인
hdfs dfs -ls /

# 사람이 읽기 쉬운 형식으로 표시 (-h 옵션)
hdfs dfs -ls -h /user/hadoop/data

# 재귀적 조회 (-R 옵션)
hdfs dfs -ls -R /user

# 디렉토리만 표시
hdfs dfs -ls -d /user/*
```

### 1.3 디렉토리 삭제

```bash
# 빈 디렉토리 삭제 (실패 예제)
hdfs dfs -mkdir /test
hdfs dfs -rmdir /test  # 성공

# 비어있지 않은 디렉토리 삭제
hdfs dfs -mkdir -p /temp/data
hdfs dfs -touchz /temp/data/file.txt
hdfs dfs -rm -r /temp  # 재귀적 삭제 (-r 옵션)

# 강제 삭제 (경고 무시)
hdfs dfs -rm -r -f /temp
```

## 실습 2: 파일 업로드 및 다운로드

### 2.1 테스트 파일 생성

```bash
# 로컬에 테스트 파일 생성
echo "Hello HDFS!" > /tmp/hello.txt
echo "This is a test file for HDFS practice." > /tmp/test.txt

# 대용량 파일 생성 (100MB)
dd if=/dev/zero of=/tmp/largefile.bin bs=1M count=100

# CSV 파일 생성
cat <<EOF > /tmp/sample.csv
id,name,age,department
1,Alice,30,Engineering
2,Bob,25,Marketing
3,Charlie,35,Sales
EOF
```

### 2.2 파일 업로드 (put)

```bash
# 단일 파일 업로드
hdfs dfs -put /tmp/hello.txt /user/hadoop/

# 여러 파일 한번에 업로드
hdfs dfs -put /tmp/test.txt /tmp/sample.csv /user/hadoop/data/

# 덮어쓰기 (-f 옵션)
hdfs dfs -put -f /tmp/hello.txt /user/hadoop/

# 로컬 파일 복사 (copyFromLocal, put과 동일)
hdfs dfs -copyFromLocal /tmp/largefile.bin /user/hadoop/
```

**진행 상황 확인**:
```bash
hdfs dfs -ls -h /user/hadoop/
```

### 2.3 파일 다운로드 (get)

```bash
# 단일 파일 다운로드
hdfs dfs -get /user/hadoop/hello.txt /tmp/downloaded_hello.txt

# 여러 파일 다운로드
hdfs dfs -get /user/hadoop/data/* /tmp/downloaded/

# 강제 덮어쓰기 (-f 옵션)
hdfs dfs -get -f /user/hadoop/hello.txt /tmp/hello.txt

# copyToLocal (get과 동일)
hdfs dfs -copyToLocal /user/hadoop/sample.csv /tmp/
```

### 2.4 파일 내용 확인

```bash
# 파일 전체 내용 출력
hdfs dfs -cat /user/hadoop/hello.txt

# 여러 파일 연결해서 출력
hdfs dfs -cat /user/hadoop/*.txt

# 파일 처음 10줄 출력
hdfs dfs -head /user/hadoop/test.txt

# 파일 마지막 10줄 출력
hdfs dfs -tail /user/hadoop/test.txt

# 파일 내용을 페이징해서 보기
hdfs dfs -cat /user/hadoop/largefile.bin | less
```

## 실습 3: 파일 복사 및 이동

### 3.1 HDFS 내부 복사

```bash
# 파일 복사
hdfs dfs -cp /user/hadoop/hello.txt /user/hadoop/hello_backup.txt

# 디렉토리 복사
hdfs dfs -cp -r /user/hadoop/data /user/hadoop/data_backup

# 확인
hdfs dfs -ls /user/hadoop/
```

### 3.2 파일 이동

```bash
# 파일 이동
hdfs dfs -mv /user/hadoop/hello.txt /user/hadoop/data/

# 여러 파일 한번에 이동
hdfs dfs -mv /user/hadoop/*.csv /user/hadoop/data/

# 디렉토리 이름 변경
hdfs dfs -mv /user/hadoop/data /user/hadoop/dataset
```

### 3.3 파일 삭제

```bash
# 파일 삭제
hdfs dfs -rm /user/hadoop/test.txt

# 여러 파일 삭제
hdfs dfs -rm /user/hadoop/*.txt

# 디렉토리 재귀 삭제
hdfs dfs -rm -r /user/hadoop/temp

# Trash 건너뛰고 즉시 삭제
hdfs dfs -rm -skipTrash /user/hadoop/largefile.bin
```

## 실습 4: 파일 권한 및 소유권

### 4.1 권한 확인

```bash
# 파일 권한 확인
hdfs dfs -ls /user/hadoop/hello.txt
```

**출력 예시**:
```
-rw-r--r--   3 root supergroup         12 2025-11-17 10:30 /user/hadoop/hello.txt
```

해석:
- `-rw-r--r--`: 권한 (소유자: 읽기+쓰기, 그룹: 읽기, 기타: 읽기)
- `3`: Replication factor
- `root`: 소유자
- `supergroup`: 그룹

### 4.2 권한 변경

```bash
# 파일 권한 변경 (chmod)
hdfs dfs -chmod 755 /user/hadoop/hello.txt

# 재귀적 권한 변경
hdfs dfs -chmod -R 644 /user/hadoop/data

# 확인
hdfs dfs -ls /user/hadoop/
```

### 4.3 소유자 및 그룹 변경

```bash
# 소유자 변경
hdfs dfs -chown hadoop /user/hadoop/hello.txt

# 소유자와 그룹 동시 변경
hdfs dfs -chown hadoop:hadoop /user/hadoop/hello.txt

# 재귀적 변경
hdfs dfs -chown -R hadoop:hadoop /user/hadoop/data
```

## 실습 5: Replication 관리

### 5.1 Replication Factor 확인

```bash
# 파일 상세 정보 확인
hdfs dfs -ls /user/hadoop/hello.txt

# 또는 stat 명령어
hdfs dfs -stat "Replication: %r" /user/hadoop/hello.txt
```

### 5.2 Replication Factor 변경

```bash
# 테스트 파일 생성
echo "Testing replication" > /tmp/replication_test.txt
hdfs dfs -put /tmp/replication_test.txt /user/hadoop/

# Replication factor를 5로 변경
hdfs dfs -setrep 5 /user/hadoop/replication_test.txt

# 확인
hdfs dfs -ls /user/hadoop/replication_test.txt

# 디렉토리 전체 변경 (재귀적)
hdfs dfs -setrep -R 3 /user/hadoop/data

# 대기 옵션 (-w): replication 완료까지 대기
hdfs dfs -setrep -w 2 /user/hadoop/replication_test.txt
```

**참고**: Replication factor는 DataNode 수를 초과할 수 없습니다.

## 실습 6: 디스크 사용량 및 통계

### 6.1 디스크 사용량 확인

```bash
# HDFS 전체 용량 확인
hdfs dfs -df -h

# 특정 디렉토리 사용량 확인
hdfs dfs -du -h /user/hadoop

# 요약 정보 (-s 옵션)
hdfs dfs -du -s -h /user/hadoop

# 사람이 읽기 쉬운 형식 (-h) + 총합 (-s)
hdfs dfs -du -s -h /user/*
```

**예상 출력**:
```
Filesystem                Size   Used  Available  Use%
hdfs://namenode:9000  100.0 G  10.5 G     89.5 G   10%
```

### 6.2 파일 통계 정보

```bash
# 파일 메타데이터
hdfs dfs -stat "Type: %F, Size: %b bytes, Replication: %r" /user/hadoop/hello.txt

# 블록 크기 확인
hdfs dfs -stat "Block size: %o" /user/hadoop/largefile.bin

# 수정 시간 확인
hdfs dfs -stat "Modified: %y" /user/hadoop/hello.txt
```

### 6.3 파일 개수 확인

```bash
# 디렉토리 내 파일/디렉토리 개수 확인
hdfs dfs -count /user/hadoop

# 사람이 읽기 쉬운 형식
hdfs dfs -count -h /user/hadoop
```

**출력 형식**:
```
DIR_COUNT   FILE_COUNT   CONTENT_SIZE   PATHNAME
```

## 실습 7: 고급 명령어

### 7.1 파일 체크섬 확인

```bash
# 파일 무결성 확인 (MD5 체크섬)
hdfs dfs -checksum /user/hadoop/hello.txt
```

### 7.2 파일 블록 정보 확인

```bash
# 파일이 저장된 블록 위치 확인
hdfs fsck /user/hadoop/largefile.bin -files -blocks -locations
```

**예상 출력**:
```
/user/hadoop/largefile.bin 104857600 bytes, 1 block(s):
 0. BP-1234567890-10.0.0.1-1234567890:blk_1073741825_1001 len=104857600 Live_repl=3
  [DatanodeInfoWithStorage[10.0.0.2:9866,DS-uuid,DISK]]
  [DatanodeInfoWithStorage[10.0.0.3:9866,DS-uuid,DISK]]
  [DatanodeInfoWithStorage[10.0.0.4:9866,DS-uuid,DISK]]
```

### 7.3 빈 파일 생성

```bash
# 빈 파일 생성 (touchz)
hdfs dfs -touchz /user/hadoop/empty.txt

# 확인
hdfs dfs -ls /user/hadoop/empty.txt
```

### 7.4 파일 병합

```bash
# 여러 파일을 하나로 병합
hdfs dfs -getmerge /user/hadoop/data/*.txt /tmp/merged.txt

# 로컬에서 확인
cat /tmp/merged.txt
```

### 7.5 파일 비교

```bash
# 두 파일 내용이 같은지 확인
hdfs dfs -test -e /user/hadoop/file1.txt && echo "Exists" || echo "Not exists"
hdfs dfs -test -z /user/hadoop/file1.txt && echo "Zero size" || echo "Has content"
hdfs dfs -test -d /user/hadoop && echo "Directory" || echo "Not directory"
```

## 실습 8: HDFS 스냅샷 (선택 사항)

### 8.1 스냅샷 활성화

```bash
# 디렉토리를 스냅샷 가능하도록 설정
hdfs dfsadmin -allowSnapshot /user/hadoop/data

# 스냅샷 생성
hdfs dfs -createSnapshot /user/hadoop/data snapshot1

# 스냅샷 목록 확인
hdfs lsSnapshottableDir
```

### 8.2 스냅샷 복원

```bash
# 파일 삭제
hdfs dfs -rm /user/hadoop/data/important.txt

# 스냅샷에서 복원
hdfs dfs -cp /user/hadoop/data/.snapshot/snapshot1/important.txt /user/hadoop/data/

# 스냅샷 삭제
hdfs dfs -deleteSnapshot /user/hadoop/data snapshot1
```

## 실습 과제

다음 과제를 직접 수행해보세요:

### 과제 1: 디렉토리 구조 생성
```bash
# 다음과 같은 구조를 만드세요:
# /project
#   ├── data
#   │   ├── raw
#   │   └── processed
#   ├── logs
#   └── results
```

<details>
<summary>정답 보기</summary>

```bash
hdfs dfs -mkdir -p /project/data/raw
hdfs dfs -mkdir -p /project/data/processed
hdfs dfs -mkdir /project/logs
hdfs dfs -mkdir /project/results
```
</details>

### 과제 2: 파일 업로드 및 복제

로컬에 3개의 파일을 생성하고, HDFS에 업로드한 후 각각 다른 replication factor를 설정하세요:
- file1.txt → replication 2
- file2.txt → replication 3
- file3.txt → replication 1

<details>
<summary>정답 보기</summary>

```bash
# 파일 생성
echo "File 1" > /tmp/file1.txt
echo "File 2" > /tmp/file2.txt
echo "File 3" > /tmp/file3.txt

# 업로드
hdfs dfs -put /tmp/file*.txt /project/data/raw/

# Replication 설정
hdfs dfs -setrep 2 /project/data/raw/file1.txt
hdfs dfs -setrep 3 /project/data/raw/file2.txt
hdfs dfs -setrep 1 /project/data/raw/file3.txt

# 확인
hdfs dfs -ls /project/data/raw/
```
</details>

### 과제 3: 디스크 사용량 분석

`/project` 디렉토리의 총 사용량과 각 하위 디렉토리별 사용량을 확인하세요.

<details>
<summary>정답 보기</summary>

```bash
# 총 사용량
hdfs dfs -du -s -h /project

# 각 하위 디렉토리 사용량
hdfs dfs -du -h /project
```
</details>

## 명령어 치트시트

| 명령어 | 설명 | 예시 |
|--------|------|------|
| `hdfs dfs -ls` | 파일/디렉토리 목록 | `hdfs dfs -ls /user` |
| `hdfs dfs -mkdir` | 디렉토리 생성 | `hdfs dfs -mkdir /data` |
| `hdfs dfs -put` | 파일 업로드 | `hdfs dfs -put file.txt /` |
| `hdfs dfs -get` | 파일 다운로드 | `hdfs dfs -get /file.txt .` |
| `hdfs dfs -cat` | 파일 내용 출력 | `hdfs dfs -cat /file.txt` |
| `hdfs dfs -cp` | HDFS 내 복사 | `hdfs dfs -cp /a.txt /b.txt` |
| `hdfs dfs -mv` | 파일 이동 | `hdfs dfs -mv /a.txt /dir/` |
| `hdfs dfs -rm` | 파일 삭제 | `hdfs dfs -rm /file.txt` |
| `hdfs dfs -rm -r` | 디렉토리 삭제 | `hdfs dfs -rm -r /dir` |
| `hdfs dfs -du` | 디스크 사용량 | `hdfs dfs -du -h /user` |
| `hdfs dfs -setrep` | Replication 설정 | `hdfs dfs -setrep 3 /file.txt` |
| `hdfs dfs -chmod` | 권한 변경 | `hdfs dfs -chmod 755 /file` |
| `hdfs dfs -chown` | 소유자 변경 | `hdfs dfs -chown user /file` |
| `hdfs fsck` | 파일 시스템 검사 | `hdfs fsck / -files -blocks` |

## 검증 체크리스트

- [ ] 디렉토리를 생성하고 삭제할 수 있음
- [ ] 파일을 HDFS에 업로드하고 다운로드할 수 있음
- [ ] 파일 내용을 cat, head, tail로 확인할 수 있음
- [ ] HDFS 내부에서 파일을 복사하고 이동할 수 있음
- [ ] Replication factor를 변경하고 확인할 수 있음
- [ ] 파일 권한과 소유자를 변경할 수 있음
- [ ] 디스크 사용량을 확인하고 분석할 수 있음
- [ ] 파일 블록 정보를 확인할 수 있음

## 다음 단계

기본 명령어를 익혔다면 실전 시나리오를 실습해보세요:
→ **[03-실전시나리오.md](./03-실전시나리오.md)**

---

**작성일**: 2025-11-17
**최종 수정**: 2025-11-17
