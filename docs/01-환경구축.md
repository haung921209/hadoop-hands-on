# 01. Hadoop HDFS 환경 구축

**태그**: #hadoop #hdfs #docker #환경구축 #hands-on
**소요 시간**: 1-2시간
**난이도**: 초급

## 목표

이 단계에서는 Docker를 사용하여 로컬 환경에 Hadoop HDFS 클러스터를 구축합니다.

**완료 기준**:
- [ ] Hadoop Docker 컨테이너 실행 성공
- [ ] HDFS 웹 UI (http://localhost:9870) 접속 성공
- [ ] HDFS 기본 명령어 실행 가능

## 방법 1: Docker Compose로 Hadoop 클러스터 구축 (권장)

### Step 1: Docker Compose 파일 생성

프로젝트 디렉토리에 `docker-compose.yml` 파일을 생성합니다:

```bash
mkdir -p ~/hadoop-practice
cd ~/hadoop-practice
```

`docker-compose.yml` 내용:

```yaml
version: '3.8'

services:
  namenode:
    image: apache/hadoop:3.3.6
    container_name: hadoop-namenode
    hostname: namenode
    command: ["hdfs", "namenode"]
    ports:
      - "9870:9870"  # NameNode Web UI
      - "9000:9000"  # HDFS Port
    environment:
      ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
    volumes:
      - namenode_data:/tmp/hadoop-root/dfs/name
    networks:
      - hadoop_network

  datanode1:
    image: apache/hadoop:3.3.6
    container_name: hadoop-datanode1
    hostname: datanode1
    command: ["hdfs", "datanode"]
    environment:
      ENSURE_DATANODE_DIR: "/tmp/hadoop-root/dfs/data"
    volumes:
      - datanode1_data:/tmp/hadoop-root/dfs/data
    networks:
      - hadoop_network
    depends_on:
      - namenode

  datanode2:
    image: apache/hadoop:3.3.6
    container_name: hadoop-datanode2
    hostname: datanode2
    command: ["hdfs", "datanode"]
    environment:
      ENSURE_DATANODE_DIR: "/tmp/hadoop-root/dfs/data"
    volumes:
      - datanode2_data:/tmp/hadoop-root/dfs/data
    networks:
      - hadoop_network
    depends_on:
      - namenode

  datanode3:
    image: apache/hadoop:3.3.6
    container_name: hadoop-datanode3
    hostname: datanode3
    command: ["hdfs", "datanode"]
    environment:
      ENSURE_DATANODE_DIR: "/tmp/hadoop-root/dfs/data"
    volumes:
      - datanode3_data:/tmp/hadoop-root/dfs/data
    networks:
      - hadoop_network
    depends_on:
      - namenode

volumes:
  namenode_data:
  datanode1_data:
  datanode2_data:
  datanode3_data:

networks:
  hadoop_network:
    driver: bridge
```

### Step 2: Hadoop 클러스터 시작

```bash
# 클러스터 시작
docker-compose up -d

# 컨테이너 상태 확인
docker-compose ps
```

**예상 출력**:
```
NAME                IMAGE               STATUS
hadoop-namenode     apache/hadoop:3.3.6 Up 2 minutes
hadoop-datanode1    apache/hadoop:3.3.6 Up 2 minutes
hadoop-datanode2    apache/hadoop:3.3.6 Up 2 minutes
hadoop-datanode3    apache/hadoop:3.3.6 Up 2 minutes
```

### Step 3: NameNode 포맷 (최초 1회만)

```bash
# NameNode 컨테이너 접속
docker exec -it hadoop-namenode bash

# NameNode 포맷
hdfs namenode -format

# 컨테이너 재시작
exit
docker-compose restart namenode
```

### Step 4: 클러스터 상태 확인

#### 4.1 웹 UI 접속
브라우저에서 다음 URL 접속:
- **NameNode UI**: http://localhost:9870
- 확인 사항:
  - Live Nodes: 3개 (datanode1, datanode2, datanode3)
  - Dead Nodes: 0개
  - Configured Capacity > 0

#### 4.2 CLI로 상태 확인

```bash
# HDFS 클러스터 리포트
docker exec hadoop-namenode hdfs dfsadmin -report

# 파일 시스템 확인
docker exec hadoop-namenode hdfs dfs -ls /
```

**정상 출력 예시**:
```
Configured Capacity: 127718608896 (118.95 GB)
Present Capacity: 107501309952 (100.12 GB)
DFS Remaining: 107501293568 (100.12 GB)
...
Live datanodes (3):
```

## 방법 2: 단일 컨테이너로 간단한 Pseudo-Distributed 모드

시간이 부족하거나 간단히 테스트하고 싶다면:

```bash
# 단일 Hadoop 컨테이너 실행
docker run -d \
  --name hadoop-standalone \
  -p 9870:9870 \
  -p 8088:8088 \
  -p 9000:9000 \
  apache/hadoop:3.3.6

# NameNode 포맷
docker exec hadoop-standalone hdfs namenode -format

# Hadoop 서비스 시작
docker exec hadoop-standalone bash -c "hdfs --daemon start namenode"
docker exec hadoop-standalone bash -c "hdfs --daemon start datanode"

# 상태 확인
docker exec hadoop-standalone hdfs dfsadmin -report
```

## 방법 3: 로컬 네이티브 설치 (선택 사항)

### macOS 설치

```bash
# Homebrew로 Hadoop 설치
brew install hadoop

# 환경 변수 설정
export HADOOP_HOME=/usr/local/Cellar/hadoop/3.3.6/libexec
export PATH=$PATH:$HADOOP_HOME/bin

# 설정 파일 수정
cd $HADOOP_HOME/etc/hadoop

# core-site.xml 수정
cat <<EOF > core-site.xml
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://localhost:9000</value>
  </property>
</configuration>
EOF

# hdfs-site.xml 수정
cat <<EOF > hdfs-site.xml
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
</configuration>
EOF

# NameNode 포맷
hdfs namenode -format

# HDFS 시작
start-dfs.sh
```

### Ubuntu/Linux 설치

```bash
# Java 설치 (필수)
sudo apt update
sudo apt install openjdk-11-jdk -y

# Hadoop 다운로드
wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
tar -xzf hadoop-3.3.6.tar.gz
sudo mv hadoop-3.3.6 /usr/local/hadoop

# 환경 변수 설정
echo 'export HADOOP_HOME=/usr/local/hadoop' >> ~/.bashrc
echo 'export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin' >> ~/.bashrc
source ~/.bashrc

# core-site.xml 및 hdfs-site.xml 설정 (macOS와 동일)
# NameNode 포맷 및 시작
hdfs namenode -format
start-dfs.sh
```

## 검증 체크리스트

환경 구축이 완료되었는지 확인하세요:

### 필수 체크리스트

- [ ] **Docker 컨테이너 실행**: `docker ps`로 hadoop-namenode, hadoop-datanode* 컨테이너 확인
- [ ] **웹 UI 접속**: http://localhost:9870 접속 성공
- [ ] **Live Nodes 확인**: 웹 UI에서 Live Nodes가 3개 표시
- [ ] **CLI 명령어 실행**: `docker exec hadoop-namenode hdfs dfs -ls /` 실행 성공
- [ ] **HDFS 리포트**: `docker exec hadoop-namenode hdfs dfsadmin -report` 정상 출력

### 선택 체크리스트

- [ ] **로그 확인**: `docker logs hadoop-namenode` 에러 없이 실행 확인
- [ ] **디렉토리 생성 테스트**: `hdfs dfs -mkdir /test` 실행 및 확인
- [ ] **파일 업로드 테스트**: 간단한 파일 업로드/다운로드 성공

## 자주 발생하는 문제 해결

### 문제 1: 포트가 이미 사용 중

**증상**:
```
Error: Port 9870 is already in use
```

**해결**:
```bash
# 포트 사용 프로세스 확인
lsof -i :9870

# 프로세스 종료
kill -9 <PID>

# 또는 docker-compose.yml에서 포트 변경
ports:
  - "19870:9870"
```

### 문제 2: DataNode가 연결되지 않음

**증상**: 웹 UI에서 Live Nodes가 0개

**해결**:
```bash
# NameNode 로그 확인
docker logs hadoop-namenode

# DataNode 로그 확인
docker logs hadoop-datanode1

# 네트워크 확인
docker network inspect hadoop-practice_hadoop_network

# 컨테이너 재시작
docker-compose restart
```

### 문제 3: 메모리 부족

**증상**:
```
Container exited with code 137
```

**해결**:
- Docker Desktop → Settings → Resources → Memory를 최소 4GB로 증가

### 문제 4: NameNode "safe mode" 상태

**증상**:
```
Name node is in safe mode
```

**해결**:
```bash
# Safe mode 수동 해제
docker exec hadoop-namenode hdfs dfsadmin -safemode leave

# Safe mode 상태 확인
docker exec hadoop-namenode hdfs dfsadmin -safemode get
```

## 유용한 명령어 모음

```bash
# 클러스터 시작/종료
docker-compose up -d          # 시작
docker-compose down           # 종료
docker-compose restart        # 재시작

# 로그 확인
docker logs -f hadoop-namenode    # NameNode 로그 (실시간)
docker logs -f hadoop-datanode1   # DataNode1 로그 (실시간)

# 컨테이너 접속
docker exec -it hadoop-namenode bash

# HDFS 상태 확인
docker exec hadoop-namenode hdfs dfsadmin -report
docker exec hadoop-namenode hdfs dfsadmin -safemode get

# 파일 시스템 확인
docker exec hadoop-namenode hdfs dfs -ls /
docker exec hadoop-namenode hdfs dfs -df -h
```

## 환경 정리 (실습 종료 시)

```bash
# 컨테이너 중지 및 삭제
docker-compose down

# 볼륨까지 완전 삭제 (주의: 모든 데이터 손실)
docker-compose down -v

# 이미지 삭제
docker rmi apache/hadoop:3.3.6
```

## 다음 단계

환경 구축이 완료되었다면 HDFS 기본 명령어를 실습해보세요:
→ **[02-기본명령어실습.md](./02-기본명령어실습.md)**

---

**작성일**: 2025-11-17
**최종 수정**: 2025-11-17
